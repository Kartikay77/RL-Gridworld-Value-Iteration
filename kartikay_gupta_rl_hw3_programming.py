# -*- coding: utf-8 -*-
"""Kartikay_Gupta_RL_HW3_Programming.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Luv7Jwz6j2DYwc-YiPj8YYUXG_QRreBG
"""

#Answer 1
import numpy as np

np.set_printoptions(precision = 4)
gridRows, gridCols = 5, 5
gamma = 0.9
delta = 0.0001

# Value Function
v = np.zeros((gridRows, gridCols))

# Rewards
r = np.array([[0, 0, 0, 0, 0],
              [0, 0, 0, 0, 0],
              [0, 0, 0, 0, 0],
              [0, 0, 0, 0, 0],
              [0, 0, -10, 0, 10]])


# State 1
s0 = (0, 0)

# Enviornment Dynamics: A Stochastic MDP
water_state = (4, 2)
goal_states = [(4, 4)]
obstacle_states = [(2, 2), (3, 2)]
actions = ['U', 'D', 'L', 'R']
print_actions = [u"\u2191", u"\u2193", u"\u2190", u"\u2192"]


# Transition function
def p(state, action):
  next_states = []
  next_state_prob = []
  rewards = []

  state_row, state_col = state[0], state[1]

  if action == 'U':
    if state_row == 0:
      nextState = state
    else:
      nextState = (state_row - 1, state_col)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.8)
    rewards.append(r[nextState[0], nextState[1]])

    if state_col == gridCols - 1:
      nextState = state
    else:
      nextState = (state_row, state_col + 1)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    if state_col == 0:
      nextState = state
    else:
      nextState = (state_row, state_col - 1)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    nextState = state
    next_states.append(nextState)
    next_state_prob.append(0.1)
    rewards.append(r[nextState[0], nextState[1]])

  if action == 'D':
    if state_row == gridRows - 1:
      nextState = state
    else:
      nextState = (state_row + 1, state_col)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.8)
    rewards.append(r[nextState[0], nextState[1]])

    if state_col == 0:
      nextState = state
    else:
      nextState = (state_row, state_col - 1)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    if state_col == gridCols - 1:
      nextState = state
    else:
      nextState = (state_row, state_col + 1)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    nextState = state
    next_states.append(nextState)
    next_state_prob.append(0.1)
    rewards.append(r[nextState[0], nextState[1]])


  if action == 'R':
    if state_col == gridCols - 1:
      nextState = state
    else:
      nextState = (state_row , state_col + 1)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.8)
    rewards.append(r[nextState[0], nextState[1]])

    if state_row == gridRows - 1:
      nextState = state
    else:
      nextState = (state_row + 1, state_col)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    if state_row == 0:
      nextState = state
    else:
      nextState = (state_row - 1, state_col)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    nextState = state
    next_states.append(nextState)
    next_state_prob.append(0.1)
    rewards.append(r[nextState[0], nextState[1]])

  if action == 'L':
    if state_col == 0:
      nextState = state
    else:
      nextState = (state_row , state_col - 1)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.8)
    rewards.append(r[nextState[0], nextState[1]])

    if state_row == 0:
      nextState = state
    else:
      nextState = (state_row - 1, state_col)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    if state_row == gridRows - 1:
      nextState = state
    else:
      nextState = (state_row + 1, state_col)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    nextState = state
    next_states.append(nextState)
    next_state_prob.append(0.1)
    rewards.append(r[nextState[0], nextState[1]])

  return next_states, next_state_prob, rewards

# Value Iteration Algorithm
iteration_counter = 0

while True:
  max_diff = 0
  v_updated = np.zeros((gridRows, gridCols))
  optimal_policy = np.zeros((gridRows, gridCols))

  for i in range(gridRows):
    for j in range(gridCols):

      s = (i, j)
      v_s = v[i][j]

      if s in goal_states:
        v_updated[i][j] = 0
      elif s in obstacle_states:
        v_updated[i][j] = 0
      else:
        temp = []
        for a in actions:
          nextStates, next_state_prob, rewards = p(s, a)
          val_update = 0
          for n in range(len(nextStates)):
            nextState = nextStates[n]
            val_update += next_state_prob[n] * (rewards[n] + gamma * v[nextState[0]][nextState[1]])
          temp.append(val_update)
        v_updated[i][j] = max(temp)
        optimal_policy[i][j] = temp.index(max(temp))
#         print(v_updated)
      max_diff = max(max_diff, abs(v_updated[i][j] - v_s))

  iteration_counter += 1
  v = v_updated
  if max_diff < delta:
    break

print('*' * 50)
print("\nNo of iterations: {} \n" .format(iteration_counter))
print('*' * 50)
print("\nValue Function: \n")
for i in range(gridRows):
  for j in range(gridCols):
    print("%.4f" %v[i][j], "  ", end='')
  print("\n")

print('*' * 50)
print("\nOptimal Policy: \n")
for i in range(gridRows):
  for j in range(gridCols):

    if (i, j) in goal_states:
      optimalAction = 'G'
    elif (i, j) in obstacle_states:
      optimalAction = ' '
    else:
      optimalAction = print_actions[int(optimal_policy[i][j])]

    print(optimalAction, "  ", end='')
  print("\n")

#Answer 2
import numpy as np

np.set_printoptions(precision = 4)
gridRows, gridCols = 5, 5
gamma = 0.25
delta = 0.0001

# Value Function
v = np.zeros((gridRows, gridCols))

# Rewards
r = np.array([[0, 0, 0, 0, 0],
              [0, 0, 0, 0, 0],
              [0, 0, 0, 0, 0],
              [0, 0, 0, 0, 0],
              [0, 0, -10, 0, 10]])


# State 1
s0 = (0, 0)

# Enviornment Dynamics: A Stochastic MDP
water_state = (4, 2)
goal_states = [(4, 4)]
obstacle_states = [(2, 2), (3, 2)]
actions = ['U', 'D', 'L', 'R']
print_actions = [u"\u2191", u"\u2193", u"\u2190", u"\u2192"]


# Transition function
def p(state, action):
  next_states = []
  next_state_prob = []
  rewards = []

  state_row, state_col = state[0], state[1]

  if action == 'U':
    if state_row == 0:
      nextState = state
    else:
      nextState = (state_row - 1, state_col)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.8)
    rewards.append(r[nextState[0], nextState[1]])

    if state_col == gridCols - 1:
      nextState = state
    else:
      nextState = (state_row, state_col + 1)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    if state_col == 0:
      nextState = state
    else:
      nextState = (state_row, state_col - 1)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    nextState = state
    next_states.append(nextState)
    next_state_prob.append(0.1)
    rewards.append(r[nextState[0], nextState[1]])

  if action == 'D':
    if state_row == gridRows - 1:
      nextState = state
    else:
      nextState = (state_row + 1, state_col)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.8)
    rewards.append(r[nextState[0], nextState[1]])

    if state_col == 0:
      nextState = state
    else:
      nextState = (state_row, state_col - 1)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    if state_col == gridCols - 1:
      nextState = state
    else:
      nextState = (state_row, state_col + 1)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    nextState = state
    next_states.append(nextState)
    next_state_prob.append(0.1)
    rewards.append(r[nextState[0], nextState[1]])


  if action == 'R':
    if state_col == gridCols - 1:
      nextState = state
    else:
      nextState = (state_row , state_col + 1)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.8)
    rewards.append(r[nextState[0], nextState[1]])

    if state_row == gridRows - 1:
      nextState = state
    else:
      nextState = (state_row + 1, state_col)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    if state_row == 0:
      nextState = state
    else:
      nextState = (state_row - 1, state_col)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    nextState = state
    next_states.append(nextState)
    next_state_prob.append(0.1)
    rewards.append(r[nextState[0], nextState[1]])

  if action == 'L':
    if state_col == 0:
      nextState = state
    else:
      nextState = (state_row , state_col - 1)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.8)
    rewards.append(r[nextState[0], nextState[1]])

    if state_row == 0:
      nextState = state
    else:
      nextState = (state_row - 1, state_col)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    if state_row == gridRows - 1:
      nextState = state
    else:
      nextState = (state_row + 1, state_col)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    nextState = state
    next_states.append(nextState)
    next_state_prob.append(0.1)
    rewards.append(r[nextState[0], nextState[1]])

  return next_states, next_state_prob, rewards

# Value Iteration Algorithm
iteration_counter = 0

while True:
  max_diff = 0
  v_updated = np.zeros((gridRows, gridCols))
  optimal_policy = np.zeros((gridRows, gridCols))

  for i in range(gridRows):
    for j in range(gridCols):

      s = (i, j)
      v_s = v[i][j]

      if s in goal_states:
        v_updated[i][j] = 0
      elif s in obstacle_states:
        v_updated[i][j] = 0
      else:
        temp = []
        for a in actions:
          nextStates, next_state_prob, rewards = p(s, a)
          val_update = 0
          for n in range(len(nextStates)):
            nextState = nextStates[n]
            val_update += next_state_prob[n] * (rewards[n] + gamma * v[nextState[0]][nextState[1]])
          temp.append(val_update)
        v_updated[i][j] = max(temp)
        optimal_policy[i][j] = temp.index(max(temp))

      max_diff = max(max_diff, abs(v_updated[i][j] - v_s))

  iteration_counter += 1
  v = v_updated
  if max_diff < delta:
    break

print('*' * 50)
print("\nNo of iterations: {} \n" .format(iteration_counter))
print('*' * 50)
print("\nValue Function: \n")
for i in range(gridRows):
  for j in range(gridCols):
    print("%.4f" %v[i][j], "  ", end='')
  print("\n")

print('*' * 50)
print("\nOptimal Policy: \n")
for i in range(gridRows):
  for j in range(gridCols):

    if (i, j) in goal_states:
      optimalAction = 'G'
    elif (i, j) in obstacle_states:
      optimalAction = ' '
    else:
      optimalAction = print_actions[int(optimal_policy[i][j])]

    print(optimalAction, "  ", end='')
  print("\n")

#Answer 3
import numpy as np

np.set_printoptions(precision = 4)
gridRows, gridCols = 5, 5
gamma = 0.9
delta = 0.0001

# Value Function
v = np.zeros((gridRows, gridCols))

# Rewards
r = np.array([[0, 0, 0, 0, 0],
              [0, 0, 0, 0, 0],
              [0, 0, 0, 0, 0],
              [0, 0, 0, 0, 0],
              [0, 0, -10, 0, 10]])


# State 1
s0 = (0, 0)

# Enviornment Dynamics: A Stochastic MDP
water_state = (4, 2)
goal_states = [(4, 4)]
obstacle_states = [(2, 2), (3, 2)]
actions = ['U', 'D', 'L', 'R']
print_actions = [u"\u2191", u"\u2193", u"\u2190", u"\u2192"]

r[0][2] = 5


# Transition function
def p(state, action):
    next_states = []
    next_state_prob = []
    rewards = []

    state_row, state_col = state[0], state[1]
    if action == 'U':
        if state_row == 0:
            nextState = state
        else:
            nextState = (state_row - 1, state_col)

        if nextState in obstacle_states: # Obstacle
            nextState = state

        next_states.append(nextState)
        next_state_prob.append(0.8)
        rewards.append(r[nextState[0], nextState[1]])

        if state_col == gridCols - 1:
            nextState = state
        else:
            nextState = (state_row, state_col + 1)

        if nextState in obstacle_states: # Obstacle
            nextState = state

        next_states.append(nextState)
        next_state_prob.append(0.05)
        rewards.append(r[nextState[0], nextState[1]])

        if state_col == 0:
            nextState = state
        else:
            nextState = (state_row, state_col - 1)

        if nextState in obstacle_states: # Obstacle
            nextState = state

        next_states.append(nextState)
        next_state_prob.append(0.05)
        rewards.append(r[nextState[0], nextState[1]])

        nextState = state
        next_states.append(nextState)
        next_state_prob.append(0.1)
        rewards.append(r[nextState[0], nextState[1]])

    if action == 'D':
        if state_row == gridRows - 1:
            nextState = state
        else:
            nextState = (state_row + 1, state_col)

        if nextState in obstacle_states: # Obstacle
            nextState = state

        next_states.append(nextState)
        next_state_prob.append(0.8)
        rewards.append(r[nextState[0], nextState[1]])

        if state_col == 0:
            nextState = state
        else:
            nextState = (state_row, state_col - 1)

        if nextState in obstacle_states: # Obstacle
            nextState = state

        next_states.append(nextState)
        next_state_prob.append(0.05)
        rewards.append(r[nextState[0], nextState[1]])

        if state_col == gridCols - 1:
            nextState = state
        else:
            nextState = (state_row, state_col + 1)

        if nextState in obstacle_states: # Obstacle
            nextState = state

        next_states.append(nextState)
        next_state_prob.append(0.05)
        rewards.append(r[nextState[0], nextState[1]])

        nextState = state
        next_states.append(nextState)
        next_state_prob.append(0.1)
        rewards.append(r[nextState[0], nextState[1]])

    if action == 'R':
        if state_col == gridCols - 1:
            nextState = state
        else:
            nextState = (state_row , state_col + 1)

        if nextState in obstacle_states: # Obstacle
            nextState = state

        next_states.append(nextState)
        next_state_prob.append(0.8)
        rewards.append(r[nextState[0], nextState[1]])

        if state_row == gridRows - 1:
            nextState = state
        else:
            nextState = (state_row + 1, state_col)

        if nextState in obstacle_states: # Obstacle
            nextState = state

        next_states.append(nextState)
        next_state_prob.append(0.05)
        rewards.append(r[nextState[0], nextState[1]])

        if state_row == 0:
            nextState = state
        else:
            nextState = (state_row - 1, state_col)

        if nextState in obstacle_states: # Obstacle
            nextState = state

        next_states.append(nextState)
        next_state_prob.append(0.05)
        rewards.append(r[nextState[0], nextState[1]])

        nextState = state
        next_states.append(nextState)
        next_state_prob.append(0.1)
        rewards.append(r[nextState[0], nextState[1]])

    if action == 'L':
        if state_col == 0:
            nextState = state
        else:
            nextState = (state_row , state_col - 1)

        if nextState in obstacle_states: # Obstacle
            nextState = state

        next_states.append(nextState)
        next_state_prob.append(0.8)
        rewards.append(r[nextState[0], nextState[1]])

        if state_row == 0:
            nextState = state
        else:
            nextState = (state_row - 1, state_col)

        if nextState in obstacle_states: # Obstacle
            nextState = state

        next_states.append(nextState)
        next_state_prob.append(0.05)
        rewards.append(r[nextState[0], nextState[1]])

        if state_row == gridRows - 1:
            nextState = state
        else:
            nextState = (state_row + 1, state_col)

        if nextState in obstacle_states: # Obstacle
            nextState = state

        next_states.append(nextState)
        next_state_prob.append(0.05)
        rewards.append(r[nextState[0], nextState[1]])

        nextState = state
        next_states.append(nextState)
        next_state_prob.append(0.1)
        rewards.append(r[nextState[0], nextState[1]])

    return next_states, next_state_prob, rewards

# Value Iteration Algorithm
iteration_counter = 0

while True:
    max_diff = 0
    v_updated = np.zeros((gridRows, gridCols))
    optimal_policy = np.zeros((gridRows, gridCols))
    for i in range(gridRows):
        for j in range(gridCols):
            s = (i, j)
            v_s = v[i][j]
            if s in goal_states:
                v_updated[i][j] = 0
            elif s in obstacle_states:
                v_updated[i][j] = 0
            else:
                temp = []
                for a in actions:
                    nextStates, next_state_prob, rewards = p(s, a)
                    val_update = 0
                    for n in range(len(nextStates)):
                        nextState = nextStates[n]
                        val_update += next_state_prob[n] * (rewards[n] + gamma * v[nextState[0]][nextState[1]])
                    temp.append(val_update)
                v_updated[i][j] = max(temp)
#                 print(v_updated)
                optimal_policy[i][j] = temp.index(max(temp))
        max_diff = max(max_diff, abs(v_updated[i][j] - v_s))
    iteration_counter += 1
    v = v_updated
    if max_diff < delta:
        break

print('*' * 50)
print("\nNo of iterations: {} \n" .format(iteration_counter))
print('*' * 50)
print("\nValue Function: \n")
for i in range(gridRows):
    for j in range(gridCols):
        print("%.4f" %v[i][j], "  ", end='')
    print("\n")

print('*' * 50)
print("\nOptimal Policy: \n")
for i in range(gridRows):
    for j in range(gridCols):

        if (i, j) in goal_states:
            optimalAction = 'G'
        elif (i, j) in obstacle_states:
            optimalAction = ' '
        else:
            optimalAction = print_actions[int(optimal_policy[i][j])]

        print(optimalAction, "  ", end='')
    print("\n")

#Answer 4a
import numpy as np

np.set_printoptions(precision = 4)
gridRows, gridCols = 5, 5
gamma = 0.9
delta = 0.0001

# Value Function
v = np.zeros((gridRows, gridCols))

# Rewards
r = np.array([[0, 0, 0, 0, 0],
              [0, 0, 0, 0, 0],
              [0, 0, 0, 0, 0],
              [0, 0, 0, 0, 0],
              [0, 0, -10, 0, 10]])


# State 1
s0 = (0, 0)

# Enviornment Dynamics: A Stochastic MDP
water_state = (4, 2)
goal_states = [(4, 4)]
obstacle_states = [(2, 2), (3, 2)]
actions = ['U', 'D', 'L', 'R']
print_actions = [u"\u2191", u"\u2193", u"\u2190", u"\u2192"]

r[0][2] = 4.999
goal_states.append((0, 2))
# Transition function
def p(state, action):
  next_states = []
  next_state_prob = []
  rewards = []

  state_row, state_col = state[0], state[1]

  if action == 'U':
    if state_row == 0:
      nextState = state
    else:
      nextState = (state_row - 1, state_col)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.8)
    rewards.append(r[nextState[0], nextState[1]])

    if state_col == gridCols - 1:
      nextState = state
    else:
      nextState = (state_row, state_col + 1)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    if state_col == 0:
      nextState = state
    else:
      nextState = (state_row, state_col - 1)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    nextState = state
    next_states.append(nextState)
    next_state_prob.append(0.1)
    rewards.append(r[nextState[0], nextState[1]])

  if action == 'D':
    if state_row == gridRows - 1:
      nextState = state
    else:
      nextState = (state_row + 1, state_col)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.8)
    rewards.append(r[nextState[0], nextState[1]])

    if state_col == 0:
      nextState = state
    else:
      nextState = (state_row, state_col - 1)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    if state_col == gridCols - 1:
      nextState = state
    else:
      nextState = (state_row, state_col + 1)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    nextState = state
    next_states.append(nextState)
    next_state_prob.append(0.1)
    rewards.append(r[nextState[0], nextState[1]])


  if action == 'R':
    if state_col == gridCols - 1:
      nextState = state
    else:
      nextState = (state_row , state_col + 1)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.8)
    rewards.append(r[nextState[0], nextState[1]])

    if state_row == gridRows - 1:
      nextState = state
    else:
      nextState = (state_row + 1, state_col)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    if state_row == 0:
      nextState = state
    else:
      nextState = (state_row - 1, state_col)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    nextState = state
    next_states.append(nextState)
    next_state_prob.append(0.1)
    rewards.append(r[nextState[0], nextState[1]])

  if action == 'L':
    if state_col == 0:
      nextState = state
    else:
      nextState = (state_row , state_col - 1)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.8)
    rewards.append(r[nextState[0], nextState[1]])

    if state_row == 0:
      nextState = state
    else:
      nextState = (state_row - 1, state_col)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    if state_row == gridRows - 1:
      nextState = state
    else:
      nextState = (state_row + 1, state_col)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    nextState = state
    next_states.append(nextState)
    next_state_prob.append(0.1)
    rewards.append(r[nextState[0], nextState[1]])

  return next_states, next_state_prob, rewards

# Value Iteration Algorithm
iteration_counter = 0

while True:
    max_diff = 0
    v_updated = np.zeros((gridRows, gridCols))
    optimal_policy = np.zeros((gridRows, gridCols))
    for i in range(gridRows):
        for j in range(gridCols):
            s = (i, j)
            v_s = v[i][j]
            if s in goal_states:
                v_updated[i][j] = 0
            elif s in obstacle_states:
                v_updated[i][j] = 0
            else:
                temp = []
                for a in actions:
                    nextStates, next_state_prob, rewards = p(s, a)
                    val_update = 0
                    for n in range(len(nextStates)):
                        nextState = nextStates[n]
                        val_update += next_state_prob[n] * (rewards[n] + gamma * v[nextState[0]][nextState[1]])
                    temp.append(val_update)
                v_updated[i][j] = max(temp)
#                 print(v_updated)
                optimal_policy[i][j] = temp.index(max(temp))
        max_diff = max(max_diff, abs(v_updated[i][j] - v_s))
    iteration_counter += 1
    v = v_updated
    if max_diff < delta:
        break

print('*' * 50)
print("\nNo of iterations: {} \n" .format(iteration_counter))
print('*' * 50)
print("\nValue Function: \n")
for i in range(gridRows):
  for j in range(gridCols):
    print("%.4f" %v[i][j], "  ", end='')
  print("\n")

print('*' * 50)
print("\nOptimal Policy: \n")
for i in range(gridRows):
  for j in range(gridCols):

    if (i, j) in goal_states:
      optimalAction = 'G'
    elif (i, j) in obstacle_states:
      optimalAction = ' '
    else:
      optimalAction = print_actions[int(optimal_policy[i][j])]

    print(optimalAction, "  ", end='')
  print("\n")

#Answer 4c
import numpy as np

np.set_printoptions(precision = 4)
gridRows, gridCols = 5, 5
gamma = 0.918
delta = 0.0001

# Value Function
v = np.zeros((gridRows, gridCols))

# Rewards
r = np.array([[0, 0, 0, 0, 0],
              [0, 0, 0, 0, 0],
              [0, 0, 0, 0, 0],
              [0, 0, 0, 0, 0],
              [0, 0, -10, 0, 10]])


# State 1
s0 = (0, 0)

# Enviornment Dynamics: A Stochastic MDP
water_state = (4, 2)
goal_states = [(4, 4)]
obstacle_states = [(2, 2), (3, 2)]
actions = ['U', 'D', 'L', 'R']
print_actions = [u"\u2191", u"\u2193", u"\u2190", u"\u2192"]

r[0][2] = 4.999
goal_states.append((0, 2))
# Transition function
def p(state, action):
  next_states = []
  next_state_prob = []
  rewards = []

  state_row, state_col = state[0], state[1]

  if action == 'U':
    if state_row == 0:
      nextState = state
    else:
      nextState = (state_row - 1, state_col)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.8)
    rewards.append(r[nextState[0], nextState[1]])

    if state_col == gridCols - 1:
      nextState = state
    else:
      nextState = (state_row, state_col + 1)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    if state_col == 0:
      nextState = state
    else:
      nextState = (state_row, state_col - 1)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    nextState = state
    next_states.append(nextState)
    next_state_prob.append(0.1)
    rewards.append(r[nextState[0], nextState[1]])

  if action == 'D':
    if state_row == gridRows - 1:
      nextState = state
    else:
      nextState = (state_row + 1, state_col)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.8)
    rewards.append(r[nextState[0], nextState[1]])

    if state_col == 0:
      nextState = state
    else:
      nextState = (state_row, state_col - 1)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    if state_col == gridCols - 1:
      nextState = state
    else:
      nextState = (state_row, state_col + 1)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    nextState = state
    next_states.append(nextState)
    next_state_prob.append(0.1)
    rewards.append(r[nextState[0], nextState[1]])


  if action == 'R':
    if state_col == gridCols - 1:
      nextState = state
    else:
      nextState = (state_row , state_col + 1)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.8)
    rewards.append(r[nextState[0], nextState[1]])

    if state_row == gridRows - 1:
      nextState = state
    else:
      nextState = (state_row + 1, state_col)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    if state_row == 0:
      nextState = state
    else:
      nextState = (state_row - 1, state_col)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    nextState = state
    next_states.append(nextState)
    next_state_prob.append(0.1)
    rewards.append(r[nextState[0], nextState[1]])

  if action == 'L':
    if state_col == 0:
      nextState = state
    else:
      nextState = (state_row , state_col - 1)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.8)
    rewards.append(r[nextState[0], nextState[1]])

    if state_row == 0:
      nextState = state
    else:
      nextState = (state_row - 1, state_col)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    if state_row == gridRows - 1:
      nextState = state
    else:
      nextState = (state_row + 1, state_col)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    nextState = state
    next_states.append(nextState)
    next_state_prob.append(0.1)
    rewards.append(r[nextState[0], nextState[1]])

  return next_states, next_state_prob, rewards

# Value Iteration Algorithm
iteration_counter = 0

while True:
    max_diff = 0
    v_updated = np.zeros((gridRows, gridCols))
    optimal_policy = np.zeros((gridRows, gridCols))
    for i in range(gridRows):
        for j in range(gridCols):
            s = (i, j)
            v_s = v[i][j]
            if s in goal_states:
                v_updated[i][j] = 0
            elif s in obstacle_states:
                v_updated[i][j] = 0
            else:
                temp = []
                for a in actions:
                    nextStates, next_state_prob, rewards = p(s, a)
                    val_update = 0
                    for n in range(len(nextStates)):
                        nextState = nextStates[n]
                        val_update += next_state_prob[n] * (rewards[n] + gamma * v[nextState[0]][nextState[1]])
                    temp.append(val_update)
                v_updated[i][j] = max(temp)
#                 print(v_updated)
                optimal_policy[i][j] = temp.index(max(temp))
        max_diff = max(max_diff, abs(v_updated[i][j] - v_s))
    iteration_counter += 1
    v = v_updated
    if max_diff < delta:
        break

print('*' * 50)
print("\nNo of iterations: {} \n" .format(iteration_counter))
print('*' * 50)
print("\nValue Function: \n")
for i in range(gridRows):
  for j in range(gridCols):
    print("%.4f" %v[i][j], "  ", end='')
  print("\n")

print('*' * 50)
print("\nOptimal Policy: \n")
for i in range(gridRows):
  for j in range(gridCols):

    if (i, j) in goal_states:
      optimalAction = 'G'
    elif (i, j) in obstacle_states:
      optimalAction = ' '
    else:
      optimalAction = print_actions[int(optimal_policy[i][j])]

    print(optimalAction, "  ", end='')
  print("\n")

#Answer 5
import numpy as np

np.set_printoptions(precision = 4)
gridRows, gridCols = 5, 5
gamma = 0.9
delta = 0.0001

# Value Function
v = np.zeros((gridRows, gridCols))

# Rewards
r = np.array([[0, 0, 0, 0, 0],
              [0, 0, 0, 0, 0],
              [0, 0, 0, 0, 0],
              [0, 0, 0, 0, 0],
              [0, 0, -10, 0, 10]])


# State 1
s0 = (0, 0)

# Enviornment Dynamics: A Stochastic MDP
water_state = (4, 2)
goal_states = [(4, 4)]
obstacle_states = [(2, 2), (3, 2)]
actions = ['U', 'D', 'L', 'R']
print_actions = [u"\u2191", u"\u2193", u"\u2190", u"\u2192"]

r[0][2] = 4.999
goal_states.append((0, 2))
# Transition function
def p(state, action):
  next_states = []
  next_state_prob = []
  rewards = []

  state_row, state_col = state[0], state[1]

  if action == 'U':
    if state_row == 0:
      nextState = state
    else:
      nextState = (state_row - 1, state_col)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.8)
    rewards.append(r[nextState[0], nextState[1]])

    if state_col == gridCols - 1:
      nextState = state
    else:
      nextState = (state_row, state_col + 1)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    if state_col == 0:
      nextState = state
    else:
      nextState = (state_row, state_col - 1)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    nextState = state
    next_states.append(nextState)
    next_state_prob.append(0.1)
    rewards.append(r[nextState[0], nextState[1]])

  if action == 'D':
    if state_row == gridRows - 1:
      nextState = state
    else:
      nextState = (state_row + 1, state_col)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.8)
    rewards.append(r[nextState[0], nextState[1]])

    if state_col == 0:
      nextState = state
    else:
      nextState = (state_row, state_col - 1)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    if state_col == gridCols - 1:
      nextState = state
    else:
      nextState = (state_row, state_col + 1)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    nextState = state
    next_states.append(nextState)
    next_state_prob.append(0.1)
    rewards.append(r[nextState[0], nextState[1]])


  if action == 'R':
    if state_col == gridCols - 1:
      nextState = state
    else:
      nextState = (state_row , state_col + 1)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.8)
    rewards.append(r[nextState[0], nextState[1]])

    if state_row == gridRows - 1:
      nextState = state
    else:
      nextState = (state_row + 1, state_col)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    if state_row == 0:
      nextState = state
    else:
      nextState = (state_row - 1, state_col)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    nextState = state
    next_states.append(nextState)
    next_state_prob.append(0.1)
    rewards.append(r[nextState[0], nextState[1]])

  if action == 'L':
    if state_col == 0:
      nextState = state
    else:
      nextState = (state_row , state_col - 1)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.8)
    rewards.append(r[nextState[0], nextState[1]])

    if state_row == 0:
      nextState = state
    else:
      nextState = (state_row - 1, state_col)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    if state_row == gridRows - 1:
      nextState = state
    else:
      nextState = (state_row + 1, state_col)

    if nextState in obstacle_states: # Obstacle
      nextState = state

    next_states.append(nextState)
    next_state_prob.append(0.05)
    rewards.append(r[nextState[0], nextState[1]])

    nextState = state
    next_states.append(nextState)
    next_state_prob.append(0.1)
    rewards.append(r[nextState[0], nextState[1]])

  return next_states, next_state_prob, rewards

# Value Iteration Algorithm
iteration_counter = 0

while True:
  diff = 0
  v_updated = np.zeros((gridRows, gridCols))
  optimal_policy = np.zeros((gridRows, gridCols))

  for i in range(gridRows):
    for j in range(gridCols):

      s = (i, j)
      v_s = v[i][j]

      if s in goal_states:
        v_updated[i][j] = 0
      elif s in obstacle_states:
        v_updated[i][j] = 0
      else:
        temp = []
        for a in actions:
          nextStates, next_state_prob, rewards = p(s, a)
          val_update = 0
          n=0
          while n<len(nextStates):
            nextState = nextStates[n]
            val_update += next_state_prob[n] * (rewards[n] + gamma * v[nextState[0]][nextState[1]])
            n+=1
          temp.append(val_update)
        v_updated[i][j] = max(temp)
        optimal_policy[i][j] = temp.index(max(temp))
#         print(v_updated)
      diff = max(diff, abs(v_updated[i][j] - v_s))

  iteration_counter += 1
  v = v_updated
  if diff < delta:
    break

print('*' * 50)
print("\nNo of iterations: {} \n" .format(iteration_counter))
print('*' * 50)
print("\nValue Function: \n")
for i in range(gridRows):
  for j in range(gridCols):
    print("%.4f" %v[i][j], "  ", end='')
  print("\n")

print('*' * 50)
print("\nOptimal Policy: \n")
for i in range(gridRows):
  for j in range(gridCols):

    if (i, j) in goal_states:
      optimalAction = 'G'
    elif (i, j) in obstacle_states:
      optimalAction = ' '
    else:
      optimalAction = print_actions[int(optimal_policy[i][j])]

    print(optimalAction, "  ", end='')
  print("\n")

